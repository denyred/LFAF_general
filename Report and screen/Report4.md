# Lexer & Scanner
## Course: Formal Languages & Finite Automata
## Author: Spataru Dionisie FAF-211





## Theory
    The term lexer refers to the process of extracting lexical tokens from a string of characters, which is also known as lexical analysis. The lexer is sometimes called a tokenizer or scanner. In the early stages of compiling or interpreting programming, markup or other types of languages, the lexer identifies tokens based on the language's rules, and these tokens are represented as lexemes. A lexeme is a result of splitting a string based on delimiters, such as spaces, while a token provides a name or category for each lexeme, but does not necessarily retain its actual value. Tokens often include additional metadata.

## Objectives:
- Understand what lexical analysis [1] is.

- Get familiar with the inner workings of a lexer/scanner/tokenizer.

- Implement a sample lexer and show how it works.
  

## Implementation description
### Lexer class
The Lexer class in Python is responsible for parsing an input string and identifying the various types of tokens present in the string. To do this, the class defines a set of regular expression patterns that match different token types such as operators, identifiers, keywords, numbers, strings, and more. When the class's tokenize method is called, it iterates over these patterns, attempting to match them against the input string. If a match is found, the corresponding token and its value are added to a list of tokens. In case an invalid token is encountered, the class raises a Invalid character exception.
```python
class Token:
    def __init__(self, type, value):
        self.type = type
        self.value = value

    def __repr__(self):
        return f"Token(type='{self.type}', value={self.value})"

class Lexer:
    def __init__(self, text):
        self.text = text
        self.pos = 0
        self.current_char = self.text[self.pos]

    def advance(self):
        self.pos += 1
        if self.pos > len(self.text) - 1:
            self.current_char = None
        else:
            self.current_char = self.text[self.pos]

    def skip_whitespace(self):
        while self.current_char is not None and self.current_char.isspace():
            self.advance()

    def integer(self):
        result = ''
        while self.current_char is not None and self.current_char.isdigit():
            result += self.current_char
            self.advance()
        return int(result)

    def get_next_token(self):
        while self.current_char is not None:

            if self.current_char.isspace():
                self.skip_whitespace()
                continue

            if self.current_char.isdigit():
                return Token('INTEGER', self.integer())

            if self.current_char == '+':
                self.advance()
                return Token('PLUS', '+')

            if self.current_char == '-':
                self.advance()
                return Token('MINUS', '-')

            if self.current_char == '*':
                self.advance()
                return Token('MUL', '*')

            if self.current_char == '/':
                self.advance()
                return Token('DIV', '/')

            if self.current_char == '(':
                self.advance()
                return Token('LPAREN', '(')

            if self.current_char == ')':
                self.advance()
                return Token('RPAREN', ')')

            if self.current_char.isalpha():
                return Token('ID', self.current_char)

            raise Exception(f"Invalid character: {self.current_char}")

        return Token('EOF', None)


```
### Tokenize method
In a lexer file, a class token represents a specific category or type of lexeme that has been identified by the lexer. Tokens contain information such as the lexeme's type and any associated metadata, but do not necessarily retain its actual value. The token class is typically used to define various types of tokens that may appear in the input source code or document, such as keywords, identifiers, operators, or literals. These tokens are then generated by the lexer as it processes the input, and passed on to subsequent stages of the compiler or interpreter for further processing.

### Main
The Main class imports the Lexer class from the lexer module, and then creates an instance of the Lexer class by invoking its constructor without any arguments. Subsequently, the tokenize method of the Lexer instance is invoked with the input string "3 + 4 * 2 - 1". The method tokenizes the input string by separating it into a list of tokens and then returns the list. Finally, the resulting list of tokens is printed to the console using the print function. The output confirms that the input string has been correctly tokenized into its component tokens, including numbers, operators, and parentheses. Moreover, the message "input valid" is printed to the console, indicating that the input string was tokenized successfully without encountering any errors.```python
```python
 # Define input text
        text = "3 + 4 * 2 - 1"


        # Create lexer object
        lexer = Lexer(text)

        token = lexer.get_next_token()

        while token.type != "EOF":
            print(token)
            token = lexer.get_next_token()

        print(token)

```



## Results
Token(type='INTEGER', value=3)
Token(type='PLUS', value=+)
Token(type='INTEGER', value=4)
Token(type='MUL', value=*)
Token(type='INTEGER', value=2)
Token(type='MINUS', value=-)
Token(type='INTEGER', value=1)
Token(type='EOF', value=None)


## Conclusions
A lexer is a crucial component of programming language processing. It breaks down an input string into tokens that represent meaningful language elements. These tokens are mapped to corresponding types and semantics defined by production rules. The lexer is used for syntax highlighting, code completion, and program analysis. This project implements a lexer in Python using regular expressions to match token types. The implementation involves iterating over the input string, matching against regular expressions, and generating corresponding tokens. Regular expressions provide an efficient way to define the language's syntax. The project demonstrates the importance of lexers in language processing and shows how they can be used to build sophisticated compilers and analysis tools.